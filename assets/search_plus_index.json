{"/pycon-2022/pages/about/": {
    "title": "About",
    "keywords": "Jekyll",
    "url": "/pycon-2022/pages/about/",
    "body": "About Bloomreach Bloomreach is the leader in Commerce Experience™ — empowering brands to deliver customer journeys so personalized, they feel like magic. Bloomreach Experience, the digital experience platform built for commerce, includes three pillars: Discovery, offering AI-driven search and merchandising; Content, offering a headless CMS; and Engagement, offering a leading CDP and marketing automation solutions. Together, these pillars form the only platform that combines the power of unified customer and product data with the speed and scale of AI-optimization, enabling measurable digital commerce experiences that drive real results. Bloomreach serves over 700 global brands including Albertsons, Bosch, Puma, FC Bayern München, and Marks &amp; Spencer, and powers over $250 billion in commerce annually. About the speakers Ondrej Unger In his first year of university, Ondrej started as a QA engineer at Exponea (now a part of Bloomreach). After five years, he is still working at Bloomreach as a backend developer in the Analytics team. The team develops and maintains Bloomreach’s own in-memory database and uses Python, C++, and Go. Ondrej also works as a team lead, taking care of development processes and the happiness of the team. Marek Bruchatý Marek is a software engineer at Bloomreach who leads the technological direction of the AI team. Marek has many years of experience in the design and development of products using artificial intelligence and Python. About the workshop We’re spoiled by the cloud. We’re so used to it that when performance problems arise, our very first thought is to scale our environment. But sometimes the best (or cheapest) option is not to increase system resources — sometimes what we need to do is dive deeply into the code and do a lot of optimization. You know, like in the old days: Just you and your profiler (and wicked amounts of caffeine). In this workshop, we’ll take one of our hiring assignments that focuses on performance. We’ll look at how our candidates usually solve the task, some of the most common mistakes and misconceptions, and the best optimization strategy."
  },"/pycon-2022/implementation/2022-09-09-05-add-tests.html": {
    "title": "05 - Add tests",
    "keywords": "implementation",
    "url": "/pycon-2022/implementation/2022-09-09-05-add-tests.html",
    "body": "BRANCH: You can start from the branch all-requests. git checkout all-requests Finally, we are going to add some tests. Add tests for fetch We are going to use very popular test framework pytest.  1. Create a test directory and test file. mkdir tests touch __init__.py touch test_fetch.py  2. Install pytest, pytest-asyncio and pytest-httpserver. pip install pytest pytest-httpserver  3. pytest-httpserver allows us to create a local server, which we will use in tests. To be able to use it, we need to modify fetch function. ... async def fetch(delay_seconds: float, url: str = BLOOMREACH_SERVER) -&gt; tuple[Success, dict]: ... We will pass the URL as parameter, so we can simply modify it. QUESTION: Do you know how this pattern is called? Click on me for the answer! This pattern is called Dependency Injection, where an object or function receives other objects or functions that it depends on.  4. Create fetch tests. import asyncio import json import time from functools import partial import pytest from pytest_httpserver import HTTPServer from werkzeug import Response from app import fetch, get_first_successful_request def slow_response(request, time_to_wait_seconds: float, status: int = 200) -&gt; Response: time.sleep(time_to_wait_seconds) return Response(json.dumps({\"time\": time_to_wait_seconds}), status, content_type=\"application/json\") @pytest.mark.asyncio async def test_successful_fetch(httpserver: HTTPServer): httpserver.expect_request(\"/foobar\").respond_with_handler(partial(slow_response, time_to_wait_seconds=0.2)) success, result = await fetch(0, httpserver.url_for(\"/foobar\")) assert success assert {\"time\": 0.2} == result @pytest.mark.asyncio async def test_unsuccessful_fetch(httpserver: HTTPServer): httpserver.expect_request(\"/foobar\").respond_with_handler( partial(slow_response, status=400, time_to_wait_seconds=0.2) ) success, result = await fetch(0, httpserver.url_for(\"/foobar\")) assert not success We created a simple handler that returns similar responses to our Bloomreach testing server. In the first test, we expect that server returns success and correct json. In the second one, we expect that success will be false.  4. Let’s create now test for first successful response. First we need to do a small refactoring. First move the code, which we would like to test, from app to a separate function. async def get_first_successful_request(unfinished_tasks: list[asyncio.Task], timeout: None | float): timeout_remaining = timeout while not out_of_time(timeout_remaining) and unfinished_tasks: start = time.monotonic() finished_tasks, unfinished_tasks = await asyncio.wait( unfinished_tasks, return_when=asyncio.FIRST_COMPLETED, timeout=timeout_remaining ) for finished_task in finished_tasks: success, result = finished_task.result() if success: return success, result end = time.monotonic() timeout_remaining = timeout_remaining - (end - start) if timeout_remaining is not None else timeout_remaining for unfinished_task in unfinished_tasks: unfinished_task.cancel() if out_of_time(timeout_remaining): raise RequestTimeout() return False, {} Then update the app function, so it uses our new function. @app.get(\"/api/smart\") async def smart_api_requester(): timeout_seconds = get_and_validate_timeout() unfinished_tasks = [ asyncio.create_task(fetch(delay_seconds=0)), asyncio.create_task(fetch(delay_seconds=WAIT_BEFORE_NEXT_REQUEST_SECONDS)), asyncio.create_task(fetch(delay_seconds=WAIT_BEFORE_NEXT_REQUEST_SECONDS)), ] success, result = await get_first_successful_request(unfinished_tasks, timeout_seconds) return jsonify(success=success, result=result)  5. We cannot use one server as we did with Bloomreach testing server. The reason is that pytest-httpserver cannot process more than one server. So, we overcome this problem by creating multiple servers. Each running in different thread. @pytest.fixture def httpserver1(httpserver_ssl_context, httpserver_listen_address): server = HTTPServer(host=HTTPServer.DEFAULT_LISTEN_HOST, port=7001, ssl_context=httpserver_ssl_context) server.start() yield server server.clear() if server.is_running(): server.stop() @pytest.fixture def httpserver2(httpserver_ssl_context, httpserver_listen_address): server = HTTPServer(host=HTTPServer.DEFAULT_LISTEN_HOST, port=7002, ssl_context=httpserver_ssl_context) server.start() yield server server.clear() if server.is_running(): server.stop()  6. The last test that we will implement will be getting the first successful response. @pytest.mark.asyncio async def test_first_successful_requests(httpserver: HTTPServer, httpserver1, httpserver2): httpserver.expect_request(\"/foo\").respond_with_handler(partial(slow_response, time_to_wait_seconds=0.8)) httpserver1.expect_request(\"/foo\").respond_with_handler(partial(slow_response, time_to_wait_seconds=0.2)) httpserver2.expect_request(\"/foo\").respond_with_handler(partial(slow_response, time_to_wait_seconds=0.4)) unfinished_tasks = [ asyncio.create_task(fetch(delay_seconds=0, url=httpserver.url_for(\"/foo\"))), asyncio.create_task(fetch(delay_seconds=0, url=httpserver1.url_for(\"/foo\"))), asyncio.create_task(fetch(delay_seconds=0, url=httpserver2.url_for(\"/foo\"))), ] success, result = await get_first_successful_request(unfinished_tasks, timeout=None) assert success assert {\"time\": 0.2} == result Each http server has same route, but with different sleep time. This means that the server with the lowest sleep time should answer first. QUESTION: Do you think we need more tests? If yes, what would they test? Click on me for the answer! We definitely miss tests that would validate if get_first_successful_request ends always in specified timeout. We also could add some tests that would validate what happens if the testing server reaches timeout or will respond with invalid content."
  },"/pycon-2022/implementation/2022-09-09-04-all-request.html": {
    "title": "04 - All request",
    "keywords": "implementation",
    "url": "/pycon-2022/implementation/2022-09-09-04-all-request.html",
    "body": "BRANCH: You can start from the branch timeout. git checkout timeout We were able to trigger the first request and if it has not finished in the specified time, we will raise the timeout error. Now, we are going to implement the second and the third request. Those two requests should be triggered after 300ms from the first one. In addition, they should be triggered at once, and we should return the first successful. If the first request finishes after 300ms, but before the second and third, we will return it. Implementing the second and third request As you might guess, all our requests will be done asynchronously. We need to secure that the second and the third request is triggered after 300ms after the first one.  1. We are going to postpone the fetch by a specific time. async def fetch(delay_seconds: float) -&gt; tuple[Success, dict]: await asyncio.sleep(delay_seconds) ... QUESTION: Why we used asyncio.sleep and no classic one time.sleep? Click on me for the answer! When time.sleep(x) is called, it will block the entire execution of the script, and it will be put on hold, just frozen, doing nothing. But when you call await asyncio.sleep(x), it will ask the event loop to run something else while your await statement finishes its execution.  2. Create tasks. @app.get(\"/api/smart\") async def smart_api_requester(): timeout_seconds = get_and_validate_timeout() unfinished_tasks = [ asyncio.create_task(fetch(delay_seconds=0)), asyncio.create_task(fetch(delay_seconds=WAIT_BEFORE_NEXT_REQUEST_SECONDS)), asyncio.create_task(fetch(delay_seconds=WAIT_BEFORE_NEXT_REQUEST_SECONDS)), ] ... As we can see, we created 3 tasks. The first request won’t be delayed, the second and third one will be delayed for the time specified in the assignment.  3. Execute those task. def out_of_time(timeout: None | float) -&gt; bool: return timeout is not None and timeout &lt;= 0 @app.get(\"/api/smart\") async def smart_api_requester(): timeout_seconds = get_and_validate_timeout() unfinished_tasks = [ asyncio.create_task(fetch(delay_seconds=0)), asyncio.create_task(fetch(delay_seconds=WAIT_BEFORE_NEXT_REQUEST_SECONDS)), asyncio.create_task(fetch(delay_seconds=WAIT_BEFORE_NEXT_REQUEST_SECONDS)), ] timeout_remaining = timeout_seconds while not out_of_time(timeout_remaining) and unfinished_tasks: start = time.monotonic() finished_tasks, unfinished_tasks = await asyncio.wait( unfinished_tasks, return_when=asyncio.FIRST_COMPLETED, timeout=timeout_remaining ) for finished_task in finished_tasks: success, result = finished_task.result() if success: return jsonify(success=success, response=result) end = time.monotonic() timeout_remaining = timeout_remaining - (end - start) if timeout_remaining is not None else timeout_remaining if out_of_time(timeout_remaining): raise RequestTimeout() return jsonify(success=False, response={}) We wait for the first task to finish. However, this task could finish with an error or with not successful response. Therefor, we need to repeat our loop till we don’t receive the first successful response, or we are not out of time. QUESTION: There is still one problem in how we handle our tasks, can you spot the problem? Click on me for the answer! If we got successful response, we will return it, but there might still be two tasks in the event loop. As we don’t need the result from them, we can cancel them. Cancel unfinished tasks ... for unfinished_task in unfinished_tasks: unfinished_task.cancel() if out_of_time(timeout_remaining): raise RequestTimeout() ..."
  },"/pycon-2022/implementation/2022-09-09-03-timeout.html": {
    "title": "03 - Timeout",
    "keywords": "implementation",
    "url": "/pycon-2022/implementation/2022-09-09-03-timeout.html",
    "body": "BRANCH: You can start from the branch first-request. git checkout first-request The client of our API can specify a timeout. The endpoint should always end in that timeout. If the timeout is not specified we will wait for the first successful response. Implementing the timeout functionality Get the timeout parameter from the request. Import required packages. from flask import Flask, jsonify, redirect, url_for, request from http import HTTPStatus Get the timeout from request args. @app.get(\"/api/smart\") def smart_api_requester(): timeout = request.args[\"timeout\"] ... QUESTION: Do you spot any problems? Click on me for the answer! We are missing validation. Let’s add it. def milliseconds_to_seconds(value: int) -&gt; float: return value / 1000 def get_and_validate_timeout() -&gt; None | float: timeout = request.args.get(\"timeout\") if timeout is None: return timeout try: converted_timeout = milliseconds_to_seconds(int(timeout)) except ValueError: raise BadRequest(\"Timeout has to be integer value.\") if converted_timeout &lt;= 0: raise BadRequest(\"Timeout has to be positive non zero value.\") return converted_timeout @app.get(\"/api/smart\") def smart_api_requester(): timeout_seconds = get_and_validate_timeout() Test timeout arguments. http://127.0.0.1:8081/api/smart?timeout=0 http://127.0.0.1:8081/api/smart?timeout=nonumber http://127.0.0.1:8081/api/smart?timeout=10 The first two requests should fail with the bad request status code. The last one should pass. End after the timeout is reached. QUESTION: How could we implement this? We need to wait for the response and also check if the timeout is not reached. Click on me for the answer! We can use asynchronous features of python. This allows you to write concurrent code. QUESTION: What is the difference between parallelism and concurrency? Is it better to use asynchronous framework instead of multiprocessing or threading? Click on me for the answer! Parallelism consists of performing multiple operations at the same time. Concurrency is a slightly broader term than parallelism. It suggests that multiple tasks have the ability to run in an overlapping manner. (There’s a saying that concurrency does not imply parallelism.) Threads consume more memory because they need to have their own stack. We also need to secure the thread safety. The same thing goes with multiprocessing. Multiprocessing works well if we have multiple CPUs and our processes are independent - we don’t need to provide thread safety. On the other hand, async is ideal if we have many I/O operations, e.g. waiting for server to respond, multiple writes into the file and so on. Source: Async IO python So, let’s create an async program. First we need to install flask with async, so our server can be run asynchronously. pip install flask[async]==2.2.2 After that, we need to import required packages. import asyncio from aiohttp import ClientSession Create the async request. Success: TypeAlias = bool async def fetch() -&gt; tuple[Success, dict]: async with ClientSession() as session: try: async with session.get(BLOOMREACH_SERVER) as response: if response.status == HTTPStatus.OK: try: result_data = await response.json() except ContentTypeError: return False, {} return True, result_data else: return False, {} except Exception: # Catch any session error (e.g. timeout) return False, {} Call async request from our route. @app.get(\"/api/smart\") async def smart_api_requester(): timeout_seconds = get_and_validate_timeout() try: success, result = await asyncio.wait_for(fetch(), timeout=timeout_seconds) except asyncio.TimeoutError: raise RequestTimeout() return jsonify(success=success, response=result)"
  },"/pycon-2022/implementation/2022-09-09-02-first-request.html": {
    "title": "02 - First Request",
    "keywords": "implementation",
    "url": "/pycon-2022/implementation/2022-09-09-02-first-request.html",
    "body": "BRANCH: You can start from the branch start. git checkout start It is always good to split complex problem into multiple smaller ones. We will do the same thing with our assignment. So, let’s now focus only on the first request. Triggering the first request To get the response from our testing server we can use python library requests. Import the requests library. import requests Get the response from our testing server. @app.get(\"/api/smart\") def smart_api_requester(): response = requests.get(BLOOMREACH_SERVER) Check the status code of the response. @app.get(\"/api/smart\") def smart_api_requester(): response = requests.get(BLOOMREACH_SERVER) if response.status_code == HTTPStatus.OK: pass Based on the status code, return a response from our endpoint. @app.get(\"/api/smart\") def smart_api_requester(): response = requests.get(BLOOMREACH_SERVER) if response.status_code == HTTPStatus.OK: return jsonify(success=True, response=response.json()) else: return jsonify(success=False)"
  },"/pycon-2022/implementation/2022-09-09-01-start.html": {
    "title": "01 - Start",
    "keywords": "implementation",
    "url": "/pycon-2022/implementation/2022-09-09-01-start.html",
    "body": "Clone the following repository: git clone https://github.com/UOndro/pycon-2022.git Then checkout start branch git checkout start ` We get a very basic skeleton of our application without the actual implementation. In our workshop, we will try to finish the implementation so all the cases in our assignment is covered. Setting up the python environment Create python virtual environment python3.10 -m venv venv Activate python virtual environment source venv/bin/activate Install requirements pip install -r requirements.txt QUESTION: Why is it good to create and use the virtual environment? Click on me for the answer! Using the virtual environment allows you to have different dependencies for each project. This means you can have different version of python packages. In addition to that, you can easily add or remove a packages without affecting other projects. Test the application After successfully setting your environment, you should be able to start your application. python app.py QUESTION: For running our server we picked web framework Flask. Do you know any python alternatives to Flask? Does this alternative have any advantages/disadvantages compared to Flask? Click on me for the answer! There are many python web frameworks, e.g. FastAPI, Django, Starlette, and so on. Each of them has different advantages/disadvantages, e.g. Starlette is ideal for async programs. FastAPI also uses Starlette, but also provide more functionality. I picked flask, because we use it in Bloomreach."
  },"/pycon-2022/assignment/2022-09-09-00-assigment.html": {
    "title": "00 - Assignment",
    "keywords": "Assignment",
    "url": "/pycon-2022/assignment/2022-09-09-00-assigment.html",
    "body": "The Task Description Write an HTTP server. You can choose to implement it either in Python or Go. The server should serve one endpoint: /api/smart Requirements for the endpoint: The endpoint performs up to 3 HTTP requests to the Exponea Testing HTTP Server and returns the first successful response. Instead of firing all 3 requests at once, the endpoint fires only a single request at the beginning. If there is a successful response within 300 milliseconds, the endpoint returns this response and does not fire other requests. If there is no successful response within 300 milliseconds, it fires another 2 requests. Then it returns the first successful response from any of the 3 requests (including the first one). The endpoint accepts a timeout parameter - an integer value specifying time in milliseconds. The endpoint always returns a response within the given timeout. If the timeout is not specified, the endpoint should respond with the first successful response from our testing server. If there is no successful response within this timeout, the endpoint returns an error. Notes The endpoint returns a JSON response. A successful response is defined as a response that returns HTTP status code 200 and has a valid payload. Your endpoints should return a JSON response that will contain successful responses from Bloomreach testing server. Bloomreach Testing HTTP Server The Bloomreach Testing HTTP Server serves a single endpoint: GET https://exponea-engineering-assignment.appspot.com/api/work It performs some work (taking roughly 100-600 ms) and then returns the time it took to respond as a JSON response. Example response: {“time”: 160} Note: The server is not very reliable , which means that it does not always return a successful response. What we seek Performance - we expect that the requests to ExponeaTesting HTTP Server will be done concurrently. We will test how your server implementation behaves under load - both in terms of requests per second, and the number of concurrent requests. Robust implementation - your server should behave correctly in presence of errors. Exponea Testing HTTP Server is infamously known for being unreliable. Also, the implementation should not leak resources (stuck threads / goroutines / connections for unknown time). Readable, maintainable implementation - for example, prefer well-known open source libraries, with licenses that allow them to be used commercially. If you want to shine… Is the task too easy for you? Don’t worry, we have prepared a few extras for you: Write tests for your implementation. Instrument your code with tools that will make debugging production easier - logging, monitoring, tracing. We need a way to check if Bloomreach Testing HTTP Server is behaving well in production. Include Dockerfile that can be used to build and run the server. Attach a discussion about code behavior - knowledge cases, how it behaves in certain conditions, performance characteristics, resource requirements, etc. How many concurrent requests can the server handle? How would you protect the server against being overloaded?"
  }}
